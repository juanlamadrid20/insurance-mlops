{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cd3fb4e-ab5c-486b-a8e0-85f6a9d17292",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "import mlflow.pyfunc\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType, FloatType, DoubleType\n",
    "\n",
    "class HealthcareBatchInferenceV2:\n",
    "    \"\"\"\n",
    "    Updated batch inference class that works seamlessly with the improved model pipeline.\n",
    "    No more preprocessing mismatches!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"juan_dev.ml.healthcare_insurance_model\", model_alias=\"champion\"):\n",
    "        self.model_name = model_name\n",
    "        self.model_alias = model_alias\n",
    "        self.fe = FeatureEngineeringClient()\n",
    "        \n",
    "        # Spark optimization for batch processing\n",
    "        spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "        spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"10000\")\n",
    "    \n",
    "    def run_batch_inference(self, input_table, output_table=None):\n",
    "        \"\"\"Execute batch inference with automatic feature lookup and proper preprocessing\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Load model from Unity Catalog\n",
    "            model_uri = f\"models:/{self.model_name}@{self.model_alias}\"\n",
    "            \n",
    "            # Get model version info\n",
    "            client = mlflow.MlflowClient()\n",
    "            model_version_info = client.get_model_version_by_alias(self.model_name, self.model_alias)\n",
    "            model_version = model_version_info.version\n",
    "            \n",
    "            print(f\"Loading model version {model_version} from {model_uri}\")\n",
    "            \n",
    "            # Load input data\n",
    "            input_df = spark.table(input_table)\n",
    "            print(f\"Input data shape: {input_df.count()} rows, {len(input_df.columns)} columns\")\n",
    "            print(f\"Input columns: {input_df.columns}\")\n",
    "            \n",
    "            # Validate required columns are present\n",
    "            required_base_columns = ['customer_id', 'sex', 'region', 'smoker', 'age', 'bmi', 'children']\n",
    "            missing_columns = [col for col in required_base_columns if col not in input_df.columns]\n",
    "            if missing_columns:\n",
    "                raise ValueError(f\"Input data is missing required columns: {missing_columns}\")\n",
    "            \n",
    "            # Ensure proper data types for the base columns\n",
    "            # The model pipeline will handle the encoding internally\n",
    "            input_df_prepared = (\n",
    "                input_df\n",
    "                .withColumn(\"sex\", upper(col(\"sex\")).cast(\"string\"))  # Ensure consistent case\n",
    "                .withColumn(\"region\", upper(col(\"region\")).cast(\"string\"))\n",
    "                .withColumn(\"smoker\", \n",
    "                           when(upper(col(\"smoker\")).isin(\"YES\", \"TRUE\", \"1\"), True)\n",
    "                           .otherwise(False).cast(\"boolean\"))\n",
    "                .withColumn(\"age\", col(\"age\").cast(\"integer\"))\n",
    "                .withColumn(\"bmi\", col(\"bmi\").cast(\"double\"))\n",
    "                .withColumn(\"children\", col(\"children\").cast(\"integer\"))\n",
    "                .withColumn(\"customer_id\", col(\"customer_id\").cast(\"long\"))\n",
    "            )\n",
    "            \n",
    "            print(\"Data preparation completed successfully\")\n",
    "            print(f\"Prepared data columns: {input_df_prepared.columns}\")\n",
    "            \n",
    "            # Batch scoring with feature engineering integration\n",
    "            # The model pipeline will automatically handle:\n",
    "            # 1. Feature lookup from juan_dev.ml.healthcare_features\n",
    "            # 2. Categorical encoding (sex, region)\n",
    "            # 3. Numerical scaling\n",
    "            # 4. Model prediction\n",
    "            print(\"Starting batch scoring with feature engineering integration...\")\n",
    "            \n",
    "            predictions_df = self.fe.score_batch(\n",
    "                df=input_df_prepared,\n",
    "                model_uri=model_uri\n",
    "            )\n",
    "            \n",
    "            print(\"Batch scoring completed successfully!\")\n",
    "            print(f\"Predictions shape: {predictions_df.count()} rows\")\n",
    "            \n",
    "            # Add business logic and metadata\n",
    "            final_predictions = (\n",
    "                predictions_df\n",
    "                .withColumn(\"prediction_timestamp\", current_timestamp())\n",
    "                .withColumn(\"model_version\", lit(model_version))\n",
    "                .withColumn(\"model_name\", lit(self.model_name))\n",
    "                \n",
    "                # Business rule: minimum charge threshold\n",
    "                .withColumn(\"adjusted_prediction\", \n",
    "                           expr(\"GREATEST(prediction, 500)\"))\n",
    "                \n",
    "                # Risk categorization for business use\n",
    "                .withColumn(\"cost_risk_category\",\n",
    "                           expr(\"CASE WHEN adjusted_prediction < 2000 THEN 'low' \" +\n",
    "                                \"WHEN adjusted_prediction < 8000 THEN 'medium' \" +\n",
    "                                \"WHEN adjusted_prediction < 20000 THEN 'high' \" +\n",
    "                                \"ELSE 'very_high' END\"))\n",
    "                \n",
    "                # Confidence intervals (approximate business rules)\n",
    "                .withColumn(\"prediction_lower_bound\", \n",
    "                           expr(\"adjusted_prediction * 0.85\"))\n",
    "                .withColumn(\"prediction_upper_bound\", \n",
    "                           expr(\"adjusted_prediction * 1.15\"))\n",
    "                \n",
    "                # Add risk flags for business decision making\n",
    "                .withColumn(\"high_risk_patient\",\n",
    "                           expr(\"adjusted_prediction > 15000 OR cost_risk_category = 'very_high'\"))\n",
    "                .withColumn(\"requires_review\", \n",
    "                           expr(\"adjusted_prediction > 25000 OR (smoker AND adjusted_prediction > 10000)\"))\n",
    "            )\n",
    "            \n",
    "            # Display results for inspection\n",
    "            print(\"Sample predictions:\")\n",
    "            final_predictions.select(\n",
    "                \"customer_id\", \"sex\", \"region\", \"smoker\", \n",
    "                \"prediction\", \"adjusted_prediction\", \"cost_risk_category\"\n",
    "            ).show(10)\n",
    "            \n",
    "            # Save results if output table specified\n",
    "            if output_table:\n",
    "                print(f\"Saving results to {output_table}...\")\n",
    "                (final_predictions\n",
    "                 .write\n",
    "                 .mode(\"overwrite\")\n",
    "                 .option(\"overwriteSchema\", \"true\")\n",
    "                 .saveAsTable(output_table))\n",
    "                print(\"Results saved successfully!\")\n",
    "            \n",
    "            # Log batch inference metrics for monitoring\n",
    "            with mlflow.start_run(run_name=\"batch_inference\"):\n",
    "                inference_count = final_predictions.count()\n",
    "                \n",
    "                # Calculate business metrics\n",
    "                avg_prediction = final_predictions.agg(avg(\"adjusted_prediction\")).collect()[0][0]\n",
    "                high_risk_count = final_predictions.filter(col(\"high_risk_patient\") == True).count()\n",
    "                requires_review_count = final_predictions.filter(col(\"requires_review\") == True).count()\n",
    "                \n",
    "                # Risk category distribution\n",
    "                risk_distribution = final_predictions.groupBy(\"cost_risk_category\").count().collect()\n",
    "                risk_dist_dict = {row.cost_risk_category: row['count'] for row in risk_distribution}\n",
    "                \n",
    "                mlflow.log_metrics({\n",
    "                    \"batch_inference_count\": inference_count,\n",
    "                    \"average_predicted_cost\": avg_prediction,\n",
    "                    \"high_risk_patient_count\": high_risk_count,\n",
    "                    \"requires_review_count\": requires_review_count,\n",
    "                    \"high_risk_percentage\": (high_risk_count / inference_count) * 100,\n",
    "                    \"model_version\": float(model_version),\n",
    "                    **{f\"risk_{k}_count\": v for k, v in risk_dist_dict.items()}\n",
    "                })\n",
    "                \n",
    "                print(f\"Logged metrics - Average predicted cost: ${avg_prediction:,.2f}\")\n",
    "                print(f\"High risk patients: {high_risk_count} ({high_risk_count/inference_count*100:.1f}%)\")\n",
    "                print(f\"Require review: {requires_review_count} ({requires_review_count/inference_count*100:.1f}%)\")\n",
    "            \n",
    "            return final_predictions\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during batch inference: {str(e)}\")\n",
    "            print(\"Troubleshooting steps:\")\n",
    "            print(\"1. Check that the model exists and has the 'champion' alias\")\n",
    "            print(\"2. Verify input data contains all required columns\")\n",
    "            print(\"3. Ensure feature table juan_dev.ml.healthcare_features is accessible\")\n",
    "            print(\"4. Check Unity Catalog permissions\")\n",
    "            raise e\n",
    "\n",
    "# Example usage with improved error handling and logging\n",
    "print(\"Initializing improved batch inference pipeline...\")\n",
    "\n",
    "batch_inference = HealthcareBatchInferenceV2()\n",
    "\n",
    "print(\"Running batch inference on insurance data...\")\n",
    "try:\n",
    "    results = batch_inference.run_batch_inference(\n",
    "        input_table=\"juan_dev.ml.insurance_silver\",\n",
    "        output_table=\"juan_dev.ml.cost_predictions\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Batch Inference Summary ===\")\n",
    "    print(f\"Successfully processed {results.count()} records\")\n",
    "    print(\"Results saved to juan_dev.ml.cost_predictions\")\n",
    "    \n",
    "    # Show sample results\n",
    "    print(\"\\n=== Sample Predictions ===\")\n",
    "    results.select(\n",
    "        \"customer_id\", \n",
    "        \"adjusted_prediction\",\n",
    "        \"cost_risk_category\",\n",
    "        \"high_risk_patient\",\n",
    "        \"requires_review\"\n",
    "    ).show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Batch inference failed: {e}\")\n",
    "    print(\"Please check the error message above and follow troubleshooting steps\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_41a50460-c90b-4840-9288-afcb847395d5",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "insurance-model-batch-v2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
