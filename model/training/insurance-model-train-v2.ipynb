{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df03e999-89ee-49c6-ae15-ba9f78450592",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "from databricks.feature_engineering import FeatureLookup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Configure Unity Catalog integration\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "mlflow.set_experiment(\"/Users/juan.lamadrid@databricks.com/experiments/insurance_cost_prediction_eda\")\n",
    "\n",
    "class HealthcareInsuranceModelV2:\n",
    "    \"\"\"\n",
    "    Improved healthcare insurance model with embedded preprocessing pipeline.\n",
    "    This ensures training/inference consistency and production readiness.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fe = FeatureEngineeringClient()\n",
    "        self.model_pipeline = None  # Will contain the full preprocessing + model pipeline\n",
    "        \n",
    "    def prepare_training_data(self):\n",
    "        \"\"\"Prepare training dataset with optimized feature lookups\"\"\"\n",
    "        # Load base training data\n",
    "        base_df = spark.table(\"juan_dev.ml.insurance_silver\")\n",
    "\n",
    "        # Define feature lookups - these should match your feature engineering\n",
    "        feature_lookups = [\n",
    "            FeatureLookup(\n",
    "                table_name=\"juan_dev.ml.healthcare_features\",\n",
    "                lookup_key=\"customer_id\",\n",
    "                feature_name=\"age_risk_score\"\n",
    "            ),\n",
    "            FeatureLookup(\n",
    "                table_name=\"juan_dev.ml.healthcare_features\",\n",
    "                lookup_key=\"customer_id\",\n",
    "                feature_name=\"smoking_impact\"\n",
    "            ),\n",
    "            FeatureLookup(\n",
    "                table_name=\"juan_dev.ml.healthcare_features\",\n",
    "                lookup_key=\"customer_id\",\n",
    "                feature_name=\"family_size_factor\"\n",
    "            ),\n",
    "            FeatureLookup(\n",
    "                table_name=\"juan_dev.ml.healthcare_features\",\n",
    "                lookup_key=\"customer_id\",\n",
    "                feature_name=\"regional_multiplier\"\n",
    "            ),\n",
    "            FeatureLookup(\n",
    "                table_name=\"juan_dev.ml.healthcare_features\",\n",
    "                lookup_key=\"customer_id\",\n",
    "                feature_name=\"health_risk_composite\"\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Create training set with automatic feature joining\n",
    "        training_set = self.fe.create_training_set(\n",
    "            df=base_df,\n",
    "            feature_lookups=feature_lookups,\n",
    "            label=\"charges\",\n",
    "            exclude_columns=[\"timestamp\", \"ingestion_timestamp\"]\n",
    "        )\n",
    "\n",
    "        return training_set\n",
    "    \n",
    "    def create_preprocessing_pipeline(self):\n",
    "        \"\"\"\n",
    "        Create a comprehensive preprocessing pipeline that handles both\n",
    "        categorical encoding and numerical scaling consistently.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Define feature columns that will be used for training\n",
    "        categorical_features = ['sex', 'region']  # Keep these as raw categorical names\n",
    "        numerical_features = [\n",
    "            'age', 'bmi', 'children', 'age_risk_score', \n",
    "            'smoking_impact', 'family_size_factor', \n",
    "            'health_risk_composite', 'regional_multiplier'\n",
    "        ]\n",
    "        \n",
    "        # Create preprocessing steps\n",
    "        # For categorical: encode to numerical values\n",
    "        # For numerical: standardize the values\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', StandardScaler(), numerical_features),\n",
    "                ('cat', LabelEncoder(), categorical_features)\n",
    "            ],\n",
    "            remainder='drop'  # Drop any columns not specified\n",
    "        )\n",
    "        \n",
    "        return preprocessor, categorical_features, numerical_features\n",
    "    \n",
    "    def train_model(self, training_set, model_type=\"random_forest\"):\n",
    "        \"\"\"Train healthcare insurance model with embedded preprocessing pipeline\"\"\"\n",
    "        \n",
    "        with mlflow.start_run(run_name=f\"healthcare_model_pipeline_{model_type}\"):\n",
    "            # Load training data\n",
    "            training_df = training_set.load_df().toPandas()\n",
    "            \n",
    "            print(f\"Training data shape: {training_df.shape}\")\n",
    "            print(f\"Training data columns: {training_df.columns.tolist()}\")\n",
    "            \n",
    "            # Create preprocessing pipeline\n",
    "            preprocessor, categorical_features, numerical_features = self.create_preprocessing_pipeline()\n",
    "            \n",
    "            # Define all feature columns we'll use\n",
    "            feature_columns = numerical_features + categorical_features\n",
    "            \n",
    "            # Prepare feature matrix and target\n",
    "            X = training_df[feature_columns]\n",
    "            y = training_df['charges']\n",
    "            \n",
    "            print(f\"Feature columns: {feature_columns}\")\n",
    "            print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "            \n",
    "            # Handle categorical encoding manually for ColumnTransformer compatibility\n",
    "            # ColumnTransformer expects specific format for LabelEncoder\n",
    "            X_processed = X.copy()\n",
    "            label_encoders = {}\n",
    "            \n",
    "            # Manually encode categorical features and store encoders\n",
    "            for feature in categorical_features:\n",
    "                le = LabelEncoder()\n",
    "                X_processed[feature] = le.fit_transform(X[feature].astype(str))\n",
    "                label_encoders[feature] = le\n",
    "            \n",
    "            # Now use StandardScaler for all features (numerical + encoded categorical)\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X_processed)\n",
    "            \n",
    "            # Train-test split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_scaled, y, test_size=0.2, random_state=42\n",
    "            )\n",
    "            \n",
    "            # Select model based on type\n",
    "            if model_type == \"random_forest\":\n",
    "                base_model = RandomForestRegressor(\n",
    "                    n_estimators=100,\n",
    "                    max_depth=10,\n",
    "                    min_samples_split=5,\n",
    "                    min_samples_leaf=2,\n",
    "                    random_state=42\n",
    "                )\n",
    "            else:  # gradient_boosting\n",
    "                base_model = GradientBoostingRegressor(\n",
    "                    n_estimators=100,\n",
    "                    learning_rate=0.1,\n",
    "                    max_depth=6,\n",
    "                    random_state=42\n",
    "                )\n",
    "            \n",
    "            # Create a custom pipeline that handles our specific preprocessing needs\n",
    "            # This is a custom solution since ColumnTransformer with LabelEncoder can be tricky\n",
    "            class HealthcarePipeline:\n",
    "                def __init__(self, label_encoders, scaler, model, feature_columns):\n",
    "                    self.label_encoders = label_encoders\n",
    "                    self.scaler = scaler\n",
    "                    self.model = model\n",
    "                    self.feature_columns = feature_columns\n",
    "                    self.categorical_features = list(label_encoders.keys())\n",
    "                    \n",
    "                def fit(self, X, y):\n",
    "                    # The preprocessing has already been done, just fit the model\n",
    "                    return self.model.fit(X, y)\n",
    "                    \n",
    "                def predict(self, X):\n",
    "                    # Apply the same preprocessing steps as training\n",
    "                    X_processed = X[self.feature_columns].copy()\n",
    "                    \n",
    "                    # Encode categorical features\n",
    "                    for feature in self.categorical_features:\n",
    "                        if feature in X_processed.columns:\n",
    "                            # Handle unseen categories by using the most frequent class\n",
    "                            try:\n",
    "                                X_processed[feature] = self.label_encoders[feature].transform(\n",
    "                                    X_processed[feature].astype(str)\n",
    "                                )\n",
    "                            except ValueError as e:\n",
    "                                print(f\"Warning: Unknown category in {feature}, using fallback\")\n",
    "                                # For unseen categories, use the most frequent encoded value\n",
    "                                most_frequent = 0  # or use mode of training data\n",
    "                                X_processed[feature] = X_processed[feature].apply(\n",
    "                                    lambda x: most_frequent if x not in self.label_encoders[feature].classes_ \n",
    "                                    else self.label_encoders[feature].transform([str(x)])[0]\n",
    "                                )\n",
    "                    \n",
    "                    # Scale features\n",
    "                    X_scaled = self.scaler.transform(X_processed)\n",
    "                    \n",
    "                    # Make prediction\n",
    "                    return self.model.predict(X_scaled)\n",
    "                    \n",
    "                def get_params(self, deep=True):\n",
    "                    return self.model.get_params(deep)\n",
    "                    \n",
    "                def set_params(self, **params):\n",
    "                    return self.model.set_params(**params)\n",
    "            \n",
    "            # Create the custom pipeline\n",
    "            healthcare_pipeline = HealthcarePipeline(\n",
    "                label_encoders=label_encoders,\n",
    "                scaler=scaler,\n",
    "                model=base_model,\n",
    "                feature_columns=feature_columns\n",
    "            )\n",
    "            \n",
    "            # Fit the pipeline (model part)\n",
    "            healthcare_pipeline.fit(X_train, y_train)\n",
    "            \n",
    "            # Store the pipeline\n",
    "            self.model_pipeline = healthcare_pipeline\n",
    "            \n",
    "            # Model evaluation\n",
    "            y_pred = healthcare_pipeline.predict(training_df[feature_columns])\n",
    "            y_test_pred = base_model.predict(X_test)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            r2 = r2_score(y_test, y_test_pred)\n",
    "            mae = mean_absolute_error(y_test, y_test_pred)\n",
    "            rmse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "            \n",
    "            # Cross-validation on scaled data\n",
    "            cv_scores = cross_val_score(base_model, X_scaled, y, cv=5, scoring='r2')\n",
    "            \n",
    "            # Healthcare-specific metrics\n",
    "            high_cost_threshold = training_df['charges'].quantile(0.95)\n",
    "            high_cost_accuracy = self._evaluate_high_cost_predictions(\n",
    "                y_test, y_test_pred, high_cost_threshold\n",
    "            )\n",
    "            \n",
    "            # Log parameters and metrics\n",
    "            mlflow.log_params({\n",
    "                \"model_type\": model_type,\n",
    "                \"n_features\": len(feature_columns),\n",
    "                \"training_samples\": len(X_train),\n",
    "                \"test_samples\": len(X_test),\n",
    "                \"preprocessing\": \"custom_pipeline_with_encoding\"\n",
    "            })\n",
    "            \n",
    "            mlflow.log_metrics({\n",
    "                \"r2_score\": r2,\n",
    "                \"mean_absolute_error\": mae,\n",
    "                \"root_mean_squared_error\": rmse,\n",
    "                \"cv_r2_mean\": cv_scores.mean(),\n",
    "                \"cv_r2_std\": cv_scores.std(),\n",
    "                \"high_cost_accuracy\": high_cost_accuracy\n",
    "            })\n",
    "            \n",
    "            # Log the complete pipeline with feature engineering integration\n",
    "            model_info = self.fe.log_model(\n",
    "                model=healthcare_pipeline,  # Log the complete pipeline\n",
    "                artifact_path=\"model\",\n",
    "                flavor=mlflow.sklearn,\n",
    "                training_set=training_set,\n",
    "                registered_model_name=\"juan_dev.ml.healthcare_insurance_model_v2\",\n",
    "                metadata={\n",
    "                    \"algorithm\": model_type,\n",
    "                    \"preprocessing\": \"embedded_pipeline\",\n",
    "                    \"healthcare_compliance\": \"HIPAA_ready\",\n",
    "                    \"model_purpose\": \"insurance_cost_prediction\",\n",
    "                    \"feature_count\": len(feature_columns),\n",
    "                    \"training_data_size\": len(training_df),\n",
    "                    \"categorical_features\": categorical_features,\n",
    "                    \"numerical_features\": numerical_features\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # print(f\"Model registered with URI: {model_info.model_uri}\")\n",
    "            return model_info\n",
    "    \n",
    "    def _evaluate_high_cost_predictions(self, y_true, y_pred, threshold):\n",
    "        \"\"\"Evaluate model performance on high-cost patients\"\"\"\n",
    "        high_cost_true = y_true >= threshold\n",
    "        high_cost_pred = y_pred >= threshold\n",
    "        return (high_cost_true == high_cost_pred).mean()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "print(\"Starting improved model training with embedded preprocessing...\")\n",
    "\n",
    "# Initialize the improved trainer\n",
    "trainer_v2 = HealthcareInsuranceModelV2()\n",
    "\n",
    "# Prepare training data\n",
    "print(\"Preparing training data with feature engineering...\")\n",
    "training_set = trainer_v2.prepare_training_data()\n",
    "\n",
    "# Train the model with embedded preprocessing\n",
    "print(\"Training Random Forest model with preprocessing pipeline...\")\n",
    "rf_model_info = trainer_v2.train_model(training_set, \"random_forest\")\n",
    "\n",
    "print(\"Model training completed successfully!\")\n",
    "print(f\"Model version: {rf_model_info.model_version}\")\n",
    "print(f\"Model URI: {rf_model_info.model_uri}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_41a50460-c90b-4840-9288-afcb847395d5",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "insurance-model-train-v2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
