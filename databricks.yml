bundle:
  name: healthcare-insurance-mlops
  
variables:
  catalog:
    description: Unity Catalog name for ML assets
    default: juan_dev
  schema:
    description: Schema name for healthcare data
    default: healthcare_data
  ml_schema:
    description: Schema name for ML assets
    default: healthcare_data
  model_name:
    description: MLflow model name
    default: insurance_model

include:
  - resources/*.yml

targets:
  dev:
    mode: development
    default: true
    workspace:
      host: https://e2-demo-field-eng.cloud.databricks.com # Replace with your workspace URL
    variables:
      catalog: juan_dev
      schema: healthcare_data
      ml_schema: healthcare_data
      model_name: insurance_model
    run_as:
      user_name: juan.lamadrid@databricks.com # Replace with your email
      
  staging:
    mode: production
    workspace:
      host: https://e2-demo-field-eng.cloud.databricks.com # Replace with your workspace URL
    variables:
      catalog: juan_dev  
      schema: healthcare_data
      ml_schema: healthcare_data
      model_name: insurance_model
    run_as:
      service_principal_name: healthcare-mlops-sp # Replace with your service principal
      
  prod:
    mode: production
    workspace:
      host: https://e2-demo-field-eng.cloud.databricks.com # Replace with your workspace URL
    variables:
      catalog: juan_prod # Separate production catalog
      schema: healthcare_data
      ml_schema: healthcare_data
      model_name: insurance_model
    run_as:
      service_principal_name: healthcare-mlops-prod-sp # Replace with your service principal

resources:
  jobs:
    feature_engineering_job:
      name: "[${bundle.target}] Healthcare Feature Engineering"
      description: "Extract and engineer features for healthcare insurance model"
      job_clusters:
        - job_cluster_key: "feature_cluster"
          new_cluster:
            spark_version: "14.3.x-cpu-ml-scala2.12"
            node_type_id: "i3.xlarge"  
            num_workers: 2
            spark_conf:
              "spark.databricks.cluster.profile": "singleNode"
              "spark.master": "local[*]"
            custom_tags:
              "project": "healthcare-insurance-mlops"
              "environment": "${bundle.target}"
              "component": "feature-engineering"
      tasks:
        - task_key: "feature_engineering"
          job_cluster_key: "feature_cluster"
          notebook_task:
            notebook_path: "./00-training/00-insurance-model-feature.ipynb"
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"
              ml_schema: "${var.ml_schema}"
          timeout_seconds: 3600
      email_notifications:
        on_failure: ["juan.lamadrid@databricks.com"] # Replace with your email
      max_concurrent_runs: 1
      
    model_training_job:
      name: "[${bundle.target}] Healthcare Model Training"
      description: "Train healthcare insurance prediction model"
      job_clusters:
        - job_cluster_key: "training_cluster"
          new_cluster:
            spark_version: "14.3.x-cpu-ml-scala2.12"
            node_type_id: "i3.xlarge"
            num_workers: 4
            spark_conf:
              "spark.databricks.adaptive.enabled": "true"
              "spark.databricks.adaptive.coalescePartitions.enabled": "true"
            custom_tags:
              "project": "healthcare-insurance-mlops"
              "environment": "${bundle.target}"
              "component": "model-training"
      tasks:
        - task_key: "model_training"
          job_cluster_key: "training_cluster"
          notebook_task:
            notebook_path: "./00-training/01-insurance-model-train.ipynb"
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"
              ml_schema: "${var.ml_schema}"
              model_name: "${var.model_name}"
          timeout_seconds: 7200
      email_notifications:
        on_failure: ["juan.lamadrid@databricks.com"] # Replace with your email
      max_concurrent_runs: 1
      
    model_governance_job:
      name: "[${bundle.target}] Healthcare Model Governance"
      description: "Validate and promote healthcare insurance model"
      job_clusters:
        - job_cluster_key: "governance_cluster"
          new_cluster:
            spark_version: "14.3.x-cpu-ml-scala2.12"
            node_type_id: "i3.xlarge"
            num_workers: 1
            custom_tags:
              "project": "healthcare-insurance-mlops"
              "environment": "${bundle.target}"
              "component": "model-governance"
      tasks:
        - task_key: "model_governance"
          job_cluster_key: "governance_cluster"
          python_wheel_task:
            package_name: "healthcare_mlops"
            entry_point: "run_governance"
            parameters: 
              - "--catalog=${var.catalog}"
              - "--schema=${var.ml_schema}"
              - "--model-name=${var.model_name}"
          libraries:
            - pypi:
                package: "mlflow>=2.8.0"
          timeout_seconds: 1800
      email_notifications:
        on_failure: ["juan.lamadrid@databricks.com"] # Replace with your email
        on_success: ["juan.lamadrid@databricks.com"] # Replace with your email
      max_concurrent_runs: 1
      
    batch_inference_job:
      name: "[${bundle.target}] Healthcare Batch Inference"
      description: "Run batch inference for healthcare insurance predictions"
      job_clusters:
        - job_cluster_key: "inference_cluster"
          new_cluster:
            spark_version: "14.3.x-cpu-ml-scala2.12"
            node_type_id: "i3.xlarge"
            num_workers: 8
            spark_conf:
              "spark.databricks.adaptive.enabled": "true"
              "spark.databricks.adaptive.coalescePartitions.enabled": "true"
              "spark.sql.execution.arrow.maxRecordsPerBatch": "10000"
            custom_tags:
              "project": "healthcare-insurance-mlops"
              "environment": "${bundle.target}"
              "component": "batch-inference"
      tasks:
        - task_key: "batch_inference"
          job_cluster_key: "inference_cluster"
          notebook_task:
            notebook_path: "./02-batch/insurance-model-batch.ipynb"
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"
              ml_schema: "${var.ml_schema}"
              model_name: "${var.model_name}"
              input_table: "${var.catalog}.${var.schema}.dim_patients"
              output_table: "${var.catalog}.${var.ml_schema}.patient_predictions"
          timeout_seconds: 3600
      schedule:
        quartz_cron_expression: "0 0 2 * * ?" # Daily at 2 AM
        timezone_id: "UTC"
        pause_status: "UNPAUSED"
      email_notifications:
        on_failure: ["juan.lamadrid@databricks.com"] # Replace with your email
      max_concurrent_runs: 1
      
    model_monitoring_job:
      name: "[${bundle.target}] Healthcare Model Monitoring"
      description: "Monitor healthcare insurance model performance and drift"
      job_clusters:
        - job_cluster_key: "monitoring_cluster"
          new_cluster:
            spark_version: "14.3.x-cpu-ml-scala2.12"
            node_type_id: "i3.xlarge"
            num_workers: 2
            custom_tags:
              "project": "healthcare-insurance-mlops"
              "environment": "${bundle.target}"
              "component": "model-monitoring"
      tasks:
        - task_key: "setup_monitoring"
          job_cluster_key: "monitoring_cluster"
          notebook_task:
            notebook_path: "./03-monitoring/insurance-model-monitor.ipynb"
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"
              ml_schema: "${var.ml_schema}"
              model_name: "${var.model_name}"
              action: "setup_monitoring"
          timeout_seconds: 1800
        - task_key: "drift_analysis"
          job_cluster_key: "monitoring_cluster"
          depends_on:
            - task_key: "setup_monitoring"
          notebook_task:
            notebook_path: "./03-monitoring/insurance-model-monitor.ipynb"
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"
              ml_schema: "${var.ml_schema}"
              model_name: "${var.model_name}"
              action: "run_drift_analysis"
          timeout_seconds: 1800
      schedule:
        quartz_cron_expression: "0 0 6 * * ?" # Daily at 6 AM
        timezone_id: "UTC"
        pause_status: "UNPAUSED"
      email_notifications:
        on_failure: ["juan.lamadrid@databricks.com"] # Replace with your email
      max_concurrent_runs: 1

    ml_training_pipeline:
      name: "[${bundle.target}] Healthcare ML Training Pipeline"
      description: "End-to-end ML training pipeline for healthcare insurance model"
      job_clusters:
        - job_cluster_key: "pipeline_cluster"
          new_cluster:
            spark_version: "14.3.x-cpu-ml-scala2.12"
            node_type_id: "i3.xlarge"
            num_workers: 4
            spark_conf:
              "spark.databricks.adaptive.enabled": "true"
            custom_tags:
              "project": "healthcare-insurance-mlops"
              "environment": "${bundle.target}"
              "component": "ml-pipeline"
      tasks:
        - task_key: "feature_engineering"
          job_cluster_key: "pipeline_cluster"
          notebook_task:
            notebook_path: "./00-training/00-insurance-model-feature.ipynb"
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"
              ml_schema: "${var.ml_schema}"
          timeout_seconds: 3600
          
        - task_key: "model_training"
          job_cluster_key: "pipeline_cluster"
          depends_on:
            - task_key: "feature_engineering"
          notebook_task:
            notebook_path: "./00-training/01-insurance-model-train.ipynb"
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"
              ml_schema: "${var.ml_schema}"
              model_name: "${var.model_name}"
          timeout_seconds: 7200
          
        - task_key: "model_governance"
          job_cluster_key: "pipeline_cluster"
          depends_on:
            - task_key: "model_training"
          python_wheel_task:
            package_name: "healthcare_mlops"
            entry_point: "run_governance"
            parameters:
              - "--catalog=${var.catalog}"
              - "--schema=${var.ml_schema}"
              - "--model-name=${var.model_name}"
          libraries:
            - pypi:
                package: "mlflow>=2.8.0"
          timeout_seconds: 1800
          
        - task_key: "batch_inference_validation"
          job_cluster_key: "pipeline_cluster"
          depends_on:
            - task_key: "model_governance"
          notebook_task:
            notebook_path: "./02-batch/insurance-model-batch.ipynb"
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"
              ml_schema: "${var.ml_schema}"
              model_name: "${var.model_name}"
              input_table: "${var.catalog}.${var.schema}.dim_patients"
              output_table: "${var.catalog}.${var.ml_schema}.patient_predictions_validation"
              validation_mode: "true"
          timeout_seconds: 3600
          
      email_notifications:
        on_failure: ["juan.lamadrid@databricks.com"] # Replace with your email
        on_success: ["juan.lamadrid@databricks.com"] # Replace with your email
      max_concurrent_runs: 1

